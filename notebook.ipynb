{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2aa090-288c-4e3c-8854-aab65fd04104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e52e7c-a186-48b8-8487-ea0f4badcd97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/',\n",
       "  'https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/',\n",
       "  'https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/',\n",
       "  'https://insights.blackcoffer.com/the-rise-of-the-ott-platform-and-its-impact-on-the-entertainment-industry-by-2040/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cyber-crime-and-its-effects/',\n",
       "  'https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040/'],\n",
       " 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Input.xlsx\")\n",
    "urls = list(df[\"URL\"])\n",
    "urls[:10], len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096584fc-3007-4b37-b77e-1f9f9abe6a72",
   "metadata": {},
   "source": [
    "# fetch_web_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5368e735-c04a-447c-9cd6-ec8e66780f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will technology eliminate the need for animal testing in drug development?\\n Imagine yourself as a being who can speak but no one can comprehend you. You are kept in captivity, in a place where you can hardly breathe. You are finally selected and taken out of that place, it is a happy moment for you. You can finally have your freedom but sadly instead of being free, you are prodded by rods and fed toxic substances against your will. The only thing you can do is scream in vain. Your voice is heard'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_web_data(url):\n",
    "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
    "    doc = requests.get(url)\n",
    "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
    "    title = soup.find(\"h1\")\n",
    "    article = soup.find_all(\"div\", {\"class\": class_[0]})\n",
    "    if article:\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    else:\n",
    "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    try:\n",
    "        start = res.index(\"Introduction\")\n",
    "        stop = res.index(\"Blackcoffer Insights\")\n",
    "    except:\n",
    "        start = 0\n",
    "        stop = -1\n",
    "    return title.text + \"\\n\" + res[start:stop]\n",
    "\n",
    "\n",
    "fetch_web_data(random.choice(urls))[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449e93b-b3b8-4f3e-865c-b09cb4cfaf79",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906f60f3-9387-4b4e-bb23-7f3db58bc35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " 'ARIARY']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words():\n",
    "    StopWords_notNames = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file != \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            res = []\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
    "            if res != []:\n",
    "                StopWords_notNames.extend(res)\n",
    "\n",
    "    StopWords_Names = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file == \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
    "                    if res != None:\n",
    "                        StopWords_Names.append(res[0])\n",
    "\n",
    "    stop_words = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        corpus = open(f\"StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
    "        res = []\n",
    "        for txt in corpus:\n",
    "            if \"|\" in txt:\n",
    "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
    "                res.append(txt.strip())\n",
    "        if res != []:\n",
    "            stop_words.extend(res)\n",
    "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
    "\n",
    "    stop_words.extend(StopWords_notNames)\n",
    "    stop_words.extend(StopWords_Names)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "get_stop_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10e19e6-e67d-4860-92fc-21cfb5033b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus impact energy markets coronavirus COVID global public health emergency source significant regional increasingly global economic disruption. impacts energy climate world ways. economic downturn puts pressure global oil prices leading Organization Petroleum Exporting Countries OPEC cuts production. hurts demand natural gas time extremely low prices. economic energy climate policymaking environment China consequential energy consumers sources greenhouse gas emissions. temporarily disrup'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_stop_words(text, personalwords=True):\n",
    "    stop_words=get_stop_words()\n",
    "    if personalwords == True:\n",
    "        stop_words.extend(nltk_sw.words(\"english\"))\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    cleaned_text = \" \".join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778b92e-42bb-4fe2-8d93-f1c093faff24",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fde7e1c-d74b-46ba-acb9-f684f08eaf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 42, -0.09090908972845338, 0.1574642123569239)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(text):\n",
    "    def get_subjectivity_score(text, positive_score, negative_score):\n",
    "   \n",
    "        # Clean and split text into words\n",
    "        cleaned_words = [word.strip().lower() for word in text.split() if word.isalpha()]\n",
    "        total_words = len(cleaned_words)\n",
    "        \n",
    "        # Avoid division by zero by adding a small constant (0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "        \n",
    "        return subjectivity_score\n",
    "\n",
    "\n",
    "    def get_polarity_score(text):\n",
    "        positive_words = (\n",
    "            open(\"MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "        negative_words = (\n",
    "            open(\"MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word.lower() in positive_words:\n",
    "                positive_score += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_score += 1\n",
    "\n",
    "        polarity_score = (positive_score - negative_score) / (\n",
    "            (positive_score + negative_score) + 0.000001\n",
    "        )\n",
    "        return polarity_score, positive_score, negative_score\n",
    "\n",
    "    \n",
    "    polarity_score, positive_score, negative_score = get_polarity_score(text)\n",
    "    subjectivity_score = get_subjectivity_score(text, positive_score, negative_score)\n",
    "    \n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "\n",
    "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6d820-5482-4b94-93b1-2813a2cd759d",
   "metadata": {},
   "source": [
    "# Analysis_of_readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165a2c03-663b-49e5-8c3e-6e96d9f2d5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,\n",
       " 7.418181818181818,\n",
       " 0.13480392156862744,\n",
       " 3.0211942959001785,\n",
       " 7.418181818181818,\n",
       " 2.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Analysis_of_readability(fetched_article):\n",
    "    sentences = fetched_article.replace(\" \", \"\").split(\".\")\n",
    "    tokens = fetched_article.split(\" \")\n",
    "    total_num_of_sentences = len(sentences)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    num_complex_words = 0\n",
    "    for token in sentences:\n",
    "        if syllables.estimate(token) > 2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
    "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "\n",
    "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
    "\n",
    "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
    "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
    "    SYLLABLE_PER_WORD\n",
    "\n",
    "    return (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    )\n",
    "\n",
    "\n",
    "Analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb50f10-e36e-4725-9fd9-b970f19bf719",
   "metadata": {},
   "source": [
    "# get_personal_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e937ae7b-0704-4872-b111-725d752ec6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6.829317269076305)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_personal_pronouns(tokens):\n",
    "    personal_pronouns = [\n",
    "        \"I\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "    ]\n",
    "    num_personal_pronouns = sum(\n",
    "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
    "    )\n",
    "\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_chars / len(tokens)\n",
    "\n",
    "    return num_personal_pronouns, avg_word_length\n",
    "\n",
    "\n",
    "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords=False)\n",
    "\n",
    "res = re.findall(\"[A-Za-z]+\", corpus)\n",
    "get_personal_pronouns(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e566d6-21d9-468d-a7ad-97faf3fe280e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T08:09:17.329442Z",
     "iopub.status.busy": "2023-05-02T08:09:17.329442Z",
     "iopub.status.idle": "2023-05-02T08:09:17.361058Z",
     "shell.execute_reply": "2023-05-02T08:09:17.361058Z",
     "shell.execute_reply.started": "2023-05-02T08:09:17.329442Z"
    },
    "tags": []
   },
   "source": [
    "<!-- # 1. All input variables in “Input.xlsx” - columns\n",
    "# 2. POSITIVE SCORE - pos_score\n",
    "# 3. NEGATIVE SCORE - neg_score\n",
    "# 4. POLARITY SCORE - Polarity_Score\n",
    "# 5. SUBJECTIVITY SCORE - Subjectivity_Score\n",
    "# 6. AVG SENTENCE LENGTH - Average_Sentence_Length\n",
    "# 7. PERCENTAGE OF COMPLEX WORDS - Percentage_of_Complex_words\n",
    "# 8. FOG INDEX - Fog_Index\n",
    "# 9. AVG NUMBER OF WORDS PER SENTENCE - Average_Number_of_Words_Per_Sentence\n",
    "# 10. COMPLEX WORD COUNT - num_complex_words\n",
    "# 11. WORD COUNT - total_num_of_words\n",
    "# 12. SYLLABLE PER WORD - SYLLABLE_PER_WORD\n",
    "# 13. PERSONAL PRONOUNS - num_personal_pronouns\n",
    "# 14. AVG WORD LENGTH - avg_word_length -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9a54e-fe67-401e-a011-35bb60dc80db",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c91fad-5b97-4dae-a792-ce2f3b80a635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blackassign0001</th>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>0.065182</td>\n",
       "      <td>6.086073</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>79</td>\n",
       "      <td>4393</td>\n",
       "      <td>1.599010</td>\n",
       "      <td>0</td>\n",
       "      <td>6.597884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0002</th>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.112772</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>7.139554</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>82</td>\n",
       "      <td>6884</td>\n",
       "      <td>1.904044</td>\n",
       "      <td>2</td>\n",
       "      <td>7.213253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0003</th>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.102740</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>0.053486</td>\n",
       "      <td>7.368763</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>56</td>\n",
       "      <td>5895</td>\n",
       "      <td>2.154728</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0004</th>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>34</td>\n",
       "      <td>68</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.178634</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>8.004654</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>52</td>\n",
       "      <td>5639</td>\n",
       "      <td>2.089595</td>\n",
       "      <td>0</td>\n",
       "      <td>7.884127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0005</th>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.089595</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>6.638823</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>41</td>\n",
       "      <td>3258</td>\n",
       "      <td>1.840708</td>\n",
       "      <td>1</td>\n",
       "      <td>7.100251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0096</th>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.291139</td>\n",
       "      <td>0.143898</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>0.045495</td>\n",
       "      <td>8.478575</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>51</td>\n",
       "      <td>4891</td>\n",
       "      <td>1.796610</td>\n",
       "      <td>1</td>\n",
       "      <td>7.018182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0097</th>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>10.764512</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>39</td>\n",
       "      <td>3587</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.420833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0098</th>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>6.214739</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>22</td>\n",
       "      <td>2005</td>\n",
       "      <td>1.837209</td>\n",
       "      <td>0</td>\n",
       "      <td>6.936255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0099</th>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>7.336161</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>35</td>\n",
       "      <td>2620</td>\n",
       "      <td>1.684375</td>\n",
       "      <td>3</td>\n",
       "      <td>6.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0100</th>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>36</td>\n",
       "      <td>54</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.161580</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>0.032498</td>\n",
       "      <td>12.321571</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>35</td>\n",
       "      <td>4831</td>\n",
       "      <td>1.818942</td>\n",
       "      <td>2</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               URL  \\\n",
       "URL_ID                                                               \n",
       "blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "...                                                            ...   \n",
       "blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "                 POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "URL_ID                                                            \n",
       "blackassign0001              31               6        0.675676   \n",
       "blackassign0002              58              25        0.397590   \n",
       "blackassign0003              38              22        0.266667   \n",
       "blackassign0004              34              68       -0.333333   \n",
       "blackassign0005              23               8        0.483871   \n",
       "...                         ...             ...             ...   \n",
       "blackassign0096              28              51       -0.291139   \n",
       "blackassign0097              22              32       -0.185185   \n",
       "blackassign0098               5               3        0.250000   \n",
       "blackassign0099              16               2        0.777778   \n",
       "blackassign0100              36              54       -0.200000   \n",
       "\n",
       "                 SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "URL_ID                                                     \n",
       "blackassign0001            0.074597            15.150000   \n",
       "blackassign0002            0.112772            17.792683   \n",
       "blackassign0003            0.102740            18.368421   \n",
       "blackassign0004            0.178634            19.961538   \n",
       "blackassign0005            0.089595            16.536585   \n",
       "...                             ...                  ...   \n",
       "blackassign0096            0.143898            21.150943   \n",
       "blackassign0097            0.125874            26.875000   \n",
       "blackassign0098            0.036866            15.480000   \n",
       "blackassign0099            0.058252            18.285714   \n",
       "blackassign0100            0.161580            30.771429   \n",
       "\n",
       "                 PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                    \n",
       "blackassign0001                     0.065182   6.086073   \n",
       "blackassign0002                     0.056203   7.139554   \n",
       "blackassign0003                     0.053486   7.368763   \n",
       "blackassign0004                     0.050096   8.004654   \n",
       "blackassign0005                     0.060472   6.638823   \n",
       "...                                      ...        ...   \n",
       "blackassign0096                     0.045495   8.478575   \n",
       "blackassign0097                     0.036279  10.764512   \n",
       "blackassign0098                     0.056848   6.214739   \n",
       "blackassign0099                     0.054688   7.336161   \n",
       "blackassign0100                     0.032498  12.321571   \n",
       "\n",
       "                 AVG NUMBER OF WORDS PER SENTENCE   COMPLEX WORD COUNT  \\\n",
       "URL_ID                                                                   \n",
       "blackassign0001                         15.150000                   79   \n",
       "blackassign0002                         17.792683                   82   \n",
       "blackassign0003                         18.368421                   56   \n",
       "blackassign0004                         19.961538                   52   \n",
       "blackassign0005                         16.536585                   41   \n",
       "...                                           ...                  ...   \n",
       "blackassign0096                         21.150943                   51   \n",
       "blackassign0097                         26.875000                   39   \n",
       "blackassign0098                         15.480000                   22   \n",
       "blackassign0099                         18.285714                   35   \n",
       "blackassign0100                         30.771429                   35   \n",
       "\n",
       "                 WORD COUNT   SYLLABLE PER WORD   PERSONAL PRONOUNS  \\\n",
       "URL_ID                                                                \n",
       "blackassign0001         4393            1.599010                  0   \n",
       "blackassign0002         6884            1.904044                  2   \n",
       "blackassign0003         5895            2.154728                  1   \n",
       "blackassign0004         5639            2.089595                  0   \n",
       "blackassign0005         3258            1.840708                  1   \n",
       "...                      ...                 ...                ...   \n",
       "blackassign0096         4891            1.796610                  1   \n",
       "blackassign0097         3587            1.560000                  1   \n",
       "blackassign0098         2005            1.837209                  0   \n",
       "blackassign0099         2620            1.684375                  3   \n",
       "blackassign0100         4831            1.818942                  2   \n",
       "\n",
       "                 AVG WORD LENGTH  \n",
       "URL_ID                            \n",
       "blackassign0001         6.597884  \n",
       "blackassign0002         7.213253  \n",
       "blackassign0003         8.000000  \n",
       "blackassign0004         7.884127  \n",
       "blackassign0005         7.100251  \n",
       "...                          ...  \n",
       "blackassign0096         7.018182  \n",
       "blackassign0097         6.420833  \n",
       "blackassign0098         6.936255  \n",
       "blackassign0099         6.375000  \n",
       "blackassign0100         7.000000  \n",
       "\n",
       "[98 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_r = []\n",
    "url_r = []\n",
    "pos_score_r = []\n",
    "neg_score_r = []\n",
    "Polarity_Score_r = []\n",
    "Polarity_Score_r = []\n",
    "Subjectivity_Score_r = []\n",
    "Average_Sentence_Length_r = []\n",
    "Percentage_of_Complex_words_r = []\n",
    "Fog_Index_r = []\n",
    "Average_Number_of_Words_Per_Sentence_r = []\n",
    "num_complex_words_r = []\n",
    "total_num_of_words_r = []\n",
    "SYLLABLE_PER_WORD_r = []\n",
    "num_personal_pronouns_r = []\n",
    "avg_word_length_r = []\n",
    "\n",
    "\n",
    "# Iterating through URLS\n",
    "for n in range(len(urls)):\n",
    "    try:\n",
    "        # fetch_web_data\n",
    "        fetched_article = fetch_web_data(urls[n])\n",
    "    except:\n",
    "        print(f\"Page {urls[n]} Not Found....!\")\n",
    "        continue\n",
    "    index = df.iloc[n]\n",
    "    id_ = index[0]\n",
    "    url_ = index[1]\n",
    "\n",
    "    # clean_stop_words\n",
    "    tokens = clean_stop_words(fetched_article)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(tokens)\n",
    "\n",
    "    (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    ) = Analysis_of_readability(fetched_article)\n",
    "\n",
    "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
    "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
    "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
    "\n",
    "    # Appending obtained variables into respective lists\n",
    "    id_r.append(id_)\n",
    "    url_r.append(url_)\n",
    "    pos_score_r.append(pos_score)\n",
    "    neg_score_r.append(neg_score)\n",
    "    Polarity_Score_r.append(Polarity_Score)\n",
    "    Subjectivity_Score_r.append(Subjectivity_Score)\n",
    "    Average_Sentence_Length_r.append(Average_Sentence_Length)\n",
    "    Percentage_of_Complex_words_r.append(Percentage_of_Complex_words)\n",
    "    Fog_Index_r.append(Fog_Index)\n",
    "    Average_Number_of_Words_Per_Sentence_r.append(Average_Number_of_Words_Per_Sentence)\n",
    "    num_complex_words_r.append(num_complex_words)\n",
    "    total_num_of_words_r.append(total_num_of_words)\n",
    "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
    "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
    "    avg_word_length_r.append(avg_word_length)\n",
    "\n",
    "\n",
    "output = {\n",
    "    \"URL_ID\": id_r,\n",
    "    \"URL\": url_r,\n",
    "    \"POSITIVE SCORE\": pos_score_r,\n",
    "    \"NEGATIVE SCORE\": neg_score_r,\n",
    "    \"POLARITY SCORE\": Polarity_Score_r,\n",
    "    \"SUBJECTIVITY SCORE\": Subjectivity_Score_r,\n",
    "    \"AVG SENTENCE LENGTH\": Average_Sentence_Length_r,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": Percentage_of_Complex_words_r,\n",
    "    \"FOG INDEX\": Fog_Index_r,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": Average_Number_of_Words_Per_Sentence_r,\n",
    "    \" COMPLEX WORD COUNT\": num_complex_words_r,\n",
    "    \"WORD COUNT \": total_num_of_words_r,\n",
    "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
    "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
    "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
    "}\n",
    "\n",
    "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111265e-aeb0-44f6-b20d-ca2207b1cba5",
   "metadata": {},
   "source": [
    "# Exporting the output_df to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f4cf4fb-5b76-4933-88e3-3c6c07f5b33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe666d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
